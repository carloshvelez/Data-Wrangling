myd("agosto de 2024 25")
ymd("2022 julio 6")
dmy ("6 de julio de 2022")
mdy ("julio seis de 2022")
ymd("2022 julio 6")
dmy ("6 de julio de 2022")
mdy ("julio 6 de 2022")
ydm("2022/06/07")
ymd("2022/06/07")
Sys.time() # esta es de R base.
now()
Sys.time() # esta es de R base.
now()
now("GMT")
now(-3)
Sys.time() # esta es de R base.
now()
now("GMT")
now("-3")
Sys.time() # esta es de R base.
now()
now("GMT")
now("-5")
Sys.time() # esta es de R base.
now()
now("GMT")
now("5")
Sys.time() # esta es de R base.
now()
now("GMT")
now("-07")
OlsonNames()
OlsonNames() %>% head()
OlsonNames() %>% head()
OlsonNames[84]
class(OlsonNames)
OlsonNames() %>% head()
OlsonNames(84)
OlsonNames() %>% head()
as.data.frame(OlsonNames())
OlsonNames() %>% head()
as.data.frame(OlsonNames())[84]
OlsonNames() %>% head()
as.data.frame(OlsonNames())[1,84]
OlsonNames() %>% head()
as.data.frame(OlsonNames())[84,1]
OlsonNames() %>% head()
colombia <- as.data.frame(OlsonNames())[84,1]
now(colombia)
hour(now())
minute(now())
secodn(now())
hour(now())
minute(now())
second(now())
hour(now())
minute(now())
second(now())
hour(now())
minute(now())
second(now())
hour(now())
minute(now())
second(now())
hora <- "11:09:25"
hms(hora)
fecha_y_hora <- "Julio 7 de 2022, 11:10:45"
mdy_hms(fecha_y_hora)
myd_hms()
library(tidyverse)
library(ggplot2)
library(lubridate)
library(tidyr)
library(scales)
set.seed(1)
install.packages("rtweet")
library(rtweet)
jsonlite::fromJSON()
trump_tweets
jsonlite::fromJSON()
data("trump_tweets")
force(trump_tweets)
View(trump_tweets)
"hola"
c("Variables incluídas", names(trump_tweets))
"Variables incluídas", names(trump_tweets)
"Variables incluídas:"
names(trump_tweets)
"Variables incluídas:"
names(trump_tweets)
"Los trinos están representados en la variable text"
head(trump_tweets$text)
"Variables incluídas:"
""
names(trump_tweets)
"Los trinos están representados en la variable text"
""
head(trump_tweets$text)
"Variables incluídas:"
""
names(trump_tweets)
""
"Los trinos están representados en la variable text"
""
head(trump_tweets$text)
"Variables incluídas:"
""
names(trump_tweets)
""
"Los trinos están representados en la variable text"
""
head(trump_tweets$text)
""
"La variable source señala el dispositivo desde el cual se compuso y subió el trino"
trump_tweets%>%select(source)%>%arrang(desc(n))
"Variables incluídas:"
""
names(trump_tweets)
""
"Los trinos están representados en la variable text"
""
head(trump_tweets$text)
""
"La variable source señala el dispositivo desde el cual se compuso y subió el trino"
trump_tweets%>%select(source)%>%arrange(desc(n))
"Variables incluídas:"
""
names(trump_tweets)
""
"Los trinos están representados en la variable text"
""
head(trump_tweets$text)
""
"La variable source señala el dispositivo desde el cual se compuso y subió el trino"
trump_tweets%>%select(source)%>%arrange(desc())
"Variables incluídas:"
""
names(trump_tweets)
""
"Los trinos están representados en la variable text"
""
head(trump_tweets$text)
""
"La variable source señala el dispositivo desde el cual se compuso y subió el trino"
trump_tweets%>%select(source)%>%arrange(desc(source))
trump_tweets %>% extract(source, "source", "Tietter for (.*)")
trump_tweets %>% extract(source, "source", "Twitter for (.*)")
View(trump_tweets)
trump_tweets %>% extract(source, "Twitter for (.*)")
trump_tweets %>% extract(source, "source", "Twitter for (.*)")
sprintf
trump_tweets %>%
extract(source, "source", "Twitter for (.*)") %>%
count(source)
trump_tweets %>%
extract(source, "source", "Twitter for (.*)")# %>%
count(source)
trump_tweets %>%
extract(source, "source", "Twitter for (.*)")# %>%
#count(source)
trump_tweets %>%
extract(source, "source", "Twitter for (.*)") %>%
count(source)
campaign_tweets <- trump_tweets %>%
extract(source, "source", "Twitter for (.*)") %>%
filter(source %in% c("Android", "iPhone") &
created_at >= ymd("2015-06-17") &
created_at < ymd("2016-11-08")) %>%
filter(!is_retweet) %>%
arrange(created_at)
campaign_tweets
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour)
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n))
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ungroup
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ungroup %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)",
y = "% of tweets",
color = "")
library(tidytext)
example <- data_frame(line = c(1, 2, 3, 4),
text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example <- data_frame(line = c(1, 2, 3, 4),
text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example %>% unnest_tokens(word, text)
unnest_tokens(word, "Estas es una palabra")
unnest_tokens(words, "Estas es una palabra")
unnest_tokens(word, "Estas es una palabra")
unnest_tokens(word, as.data.frame("Estas es una palabra"))
example %>% unnest_tokens(word, text, token = "characters")
example %>% unnest_tokens(word, text, token = "characters")%>%group_by(word)%>%sum()
example %>% unnest_tokens(word, text, token = "characters")%>%group_by(word)%>%length()
example %>% unnest_tokens(word, text, token = "characters")%>%group_by(word)%>%summary()
example %>% unnest_tokens(word, text, token = "characters")%>%group_by(word)
i <- 3005
campaign_tweets$text[i]
campaign_tweets[i,]%>%unnest_tokens(word, text) %>% select(word)
i <- 3008
campaign_tweets$text[i]
campaign_tweets[i,]%>%unnest_tokens(word, text) %>% select(word)
pattern <- "([Â-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
campaign_tweets[i,]%>%unnest_tokens(word, text, token = "regex", pattern = pattern) %>% select(word)
campaign_tweets[i,]%>%unnest_tokens(word, text, token = pattern) %>% select(word)
pattern <- "([A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
campaign_tweets[i,]%>%unnest_tokens(word, text, token = "regex", pattern = pattern) %>% select(word)
pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
campaign_tweets[i,]%>%unnest_tokens(word, text, token = "regex", pattern = pattern) %>% select(word)
campaign_tweets[i,]%>%unnest_tokens(word, text, token = "tweets") %>% select(word)
campaign_tweets[i,] %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
select(word)
campaign_tweets[i,]%>%unnest_tokens(word, text, token = "tweets") %>% select(word)
campaign_tweets[i,]%>%unnest_tokens(word, text, token = "tweets") %>% select(word)
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>% head()
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern)
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern)
tweet_words
tweet_words %>%
count(word) %>%
arrange(desc(n))
stop_words
tm::stopwords
pkgs <- c("rtweet",
"stringr",
"tidytext",
"quanteda",
"wordcloud",
"readr",
"purrr",
"dplyr",
"ggplot2")
invisible(lapply(pkgs, require, character.onle=TRUE))
invisible(lapply(pkgs, require, character.only=TRUE))
install.packages("tm")
library(tm)
tm::stopwords("es")
stop_words
stopwords("es")
?stops_words
?stop_words
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
filter(!word %in% stop_words$word )
tweet_words
tweet_words %>%
count(word) %>%
top_n(10, n) %>%
mutate(word = reorder(word, n)) %>%
arrange(desc(n))
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
tweet_words
tweet_words %>%
count(word, source)
tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0)
android_iphone_or <- tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0) %>%
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) /
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0) %>%
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) /
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0) %>%
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) /
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))%>% arrange(desc(or)
android_iphone_or <- tweet_words %>% #Crea un objeto llamado android_iphon_or
count(word, source) %>% # Cuenta las palabras y la fuente.
spread(source, n, fill = 0) %>% # crea una columna diferente para android y otra para iphone.
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / #crea una columna or con las odds ratio para android.
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
ndroid_iphone_or %>% arrange(desc(or)) # ordena descendentemente.
android_iphone_or <- tweet_words %>% #Crea un objeto llamado android_iphon_or
count(word, source) %>% # Cuenta las palabras y la fuente.
spread(source, n, fill = 0) %>% # crea una columna diferente para android y otra para iphone.
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / #crea una columna or con las odds ratio para android.
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
android_iphone_or %>% arrange(desc(or)) # ordena descendentemente.
android_iphone_or %>% arrange(or)# ordena ascendentemente.
android_iphone_or %>% filter(Android+iPhone > 100) %>%
arrange(desc(or))
android_iphone_or %>% filter(Android+iPhone > 100) %>%
arrange(or)
sentiments
get_sentiments()
get_sentiments("affin")
get_sentiments("afinn")
install("textdata")
??textdata
install.packages("textdata")
library(textdata)
get_sentiments("afinn")
get_sentiments("loughran")
get_sentiments("nrc")
nrc <- get_sentiments("nrc") %>%
select(word, sentiment)
nrc
tweet_words %>% inner_join(nrc, by = "word") %>%
select(source, word, sentiment)
sentiment_counts <- tweet_words %>%
left_join(nrc, by = "word") %>%
count(source, sentiment) %>%
spread(source, n) %>%
mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
tweet_words %>% group_by(source) %>% summarize(n = n())
sentiment_counts %>%
mutate(Android = Android / (sum(Android) - Android) ,
iPhone = iPhone / (sum(iPhone) - iPhone),
or = Android/iPhone) %>%
arrange(desc(or))
library(broom)
log_or <- sentiment_counts %>%
mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
conf.low = log_or - qnorm(0.975)*se,
conf.high = log_or + qnorm(0.975)*se) %>%
arrange(desc(log_or))
log_or
log_or %>%
mutate(sentiment = reorder(sentiment, log_or),) %>%
ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
geom_errorbar() +
geom_point(aes(sentiment, log_or)) +
ylab("Log odds ratio for association between Android and sentiment") +
coord_flip()
android_iphone_or %>% inner_join(nrc) %>%
filter(sentiment == "disgust" & Android + iPhone > 10) %>%
arrange(desc(or))
android_iphone_or %>% inner_join(nrc) %>%
filter(sentiment == "anger" & Android + iPhone > 10) %>%
arrange(desc(or))
android_iphone_or %>% inner_join(nrc) %>%
filter(sentiment == "negative" & Android + iPhone > 10) %>%
arrange(desc(or))
android_iphone_or %>% inner_join(nrc) %>%
filter(sentiment == "fear" & Android + iPhone > 10) %>%
arrange(desc(or))
android_iphone_or %>% inner_join(nrc, by = "word") %>%
mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
mutate(log_or = log(or)) %>%
filter(Android + iPhone > 10 & abs(log_or)>1) %>%
mutate(word = reorder(word, log_or)) %>%
ggplot(aes(word, log_or, fill = log_or < 0)) +
facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
geom_bar(stat="identity", show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
data("brexit_polls")
brexit_polls
month(brexit_polls$startdate)
brexit_polls %>% mutate(month = month(startdate))%>%filter(month==4)
brexit_polls %>% round_date(enddate)
brexit_polls %>% round_date(enddate)
round_date(brexit_polls$enddate)
round_date(brexit_polls$enddate, unit = "weak")
round_date(brexit_polls$enddate, unit = "week")
round_date(brexit_polls$enddate, unit = "week") == "2016-06-12"
round_date(brexit_polls$enddate, unit = "week") == "2016-06-12"%>%count()
sum(round_date(brexit_polls$enddate, unit = "week") == "2016-06-12")
brexit_polls %>% mutate(weekday = weekdays(enddate))%>%arrange(weekday)
movielens
names(movielens)
movielens %>% mutate(timestamp = as_datetime(timestamp))%>%mutate(year = year(timestamp), hour = hour(timestamp))%>% arrange(year)
movielens %>% mutate(timestamp = as_datetime(timestamp))%>%mutate(year = year(timestamp), hour = hour(timestamp))%>% arrange(hour)
movielens %>% mutate(timestamp = as_datetime(timestamp))%>%mutate(year = year(timestamp), hour = hour(timestamp))%>% arrange(count(hour))
movielens %>% mutate(timestamp = as_datetime(timestamp))%>%mutate(year = year(timestamp), hour = hour(timestamp))
datos_evaluación <- movielens %>% mutate(timestamp = as_datetime(timestamp))%>%mutate(year = year(timestamp), hour = hour(timestamp))
View(datos_evaluación)
count(datos_evaluación$year)
datos_evaluación%>%group_by(year)%>%count()
datos_evaluación%>%group_by(year)%>%count()%>%arrange(n)
datos_evaluación%>%group_by(year)%>%count()%>%arrange(desc(n))
datos_evaluación%>%group_by(hour)%>%count()%>%arrange(desc(n))
library(tidyverse)
library(gutenbergr)
library(tidytext)
options(digits = 3)
gutenberg_metadata
rm (list = ls())
libros <- gutenberg_metadata
libros$title %>% str_detect("Pride and Prejudice")
libros[libros$title %>% str_detect("Pride and Prejudice")]
libros[str_detect(libros$title, "Pride and Prejudice")]
names(libros)
libros$gutenberg_id[str_detect(libros$title, "Pride and Prejudice")]
libros <- gutenberg_works()
libros
libros$gutenberg_id[str_detect(libros$title, "Pride and Prejudice")]
libros[, 1342]
libros[1342,]
libros[37431,]
libros%>%filter(gutenberg_id %in% c(1342, 37431))
libro <- gutenberg_download("Pride and Prejudice")
libro
View(libro)
?gutenberg_download
libro <- gutenberg_download(1342)
View(libro)
words <- unnest_tokens(libro, words, palabras)
words <- unnest_tokens(libro, palabras, words)
words <- libro %>% unnest_tokens(words, palabras)
words <- libro %>% unnest_tokens(palabras, token = "words")
words <- libro %>% unnest_tokens(text, palabras, token = "words")
words <- libro %>% unnest_tokens(text, token = "words")
words <- libro %>% unnest_tokens(palabras, text, token = "words")
words
length(words$palabras)
palabras_vacías <- stop_words
words %>% filter(!palabras %in% palabras_vacías)
words %>% filter(!palabras %in% palabras_vacías)%>%count(palabras)
words %>% filter(!palabras %in% palabras_vacías)%>%length()
words %>% filter(!palabras %in% palabras_vacías)%>%length(palabras)
words %>% filter(!palabras %in% palabras_vacías)
words %>% filter(palabras !%in% palabras_vacías)
words %>% filter(!palabras %in% palabras_vacías)
words %>% filter(palabras_vacías %in% palabras)
View(palabras_vacías)
words %>% filter(palabras_vacías$word %in% palabras)
words %>% filter(palabras%in%palabras_vacías$word  )
words
words2 <- words %>% filter(!palabras%in%palabras_vacías$word)
words %>%anti_join(palabras_vacías)
words %>%anti_join(palabras_vacías by = "palabras")
words2%>%string_subset("\\D")
words2%>%str_subset(palabras, "\\D")
words2
words2$palabras%>%str_subset("\\D")
length(words2$palabras%>%str_subset("\\D"))
length(words2$palabras%>%str_subset("\\d"))
37459-128
pride <- libro %>% unnest_tokens(words, text, token = "words")
pride <- libro %>% unnest_tokens(words, text, token = "words")
## Remuevo stop_words
palabras_vacías <- stop_words
pride <- pride %>% inner_join(palabras_vacías)
pride
palabras_vacías
pride <- libro %>% unnest_tokens(word, text, token = "words")
pride <- pride %>% inner_join(palabras_vacías)
pride
pride <- pride %>% inner_join(palabras_vacías)
nrow(pride)
pride
pride <- libro %>% unnest_tokens(word, text, token = "words")
pride <- pride %>% anti_join(palabras_vacías)
nrow(pride)
pride%>%str_extract(words, "\\D")
pride[!str_detect(pride$word, "\\d")]
pride[!str_detect(pride$word, "\\d"),]
pride <- pride[!str_detect(pride$word, "\\d"),]
pride %>% grup_by(word) %>%count()
pride %>% group_by(word) %>%count()
pride %>% group_by(word) %>%count()%>%filter(n>100)
pride %>% group_by(word) %>%count()%>%filter(n>100)%>%arrange(desc(n))
afinn <- get_sentiments("afinn")
pride %>% anti_join(afinn)
pride %>% left_join(afinn)
pride %>% left_join(afinn) %>% filter(!is.na(value))
pride %>% left_join(afinn) %>% filter(!is.na(value)& value>0)
pride %>% left_join(afinn) %>% filter(!is.na(value)& value==4)
pride %>% left_join(afinn) %>% filter(!is.na(value)& value>=0)
pride %>% left_join(afinn) %>% filter(!is.na(value)& value>0)
pride %>% left_join(afinn) %>% filter(value>0)
pride %>% left_join(afinn) %>% filter(value>=0)
a <- pride %>% left_join(afinn) %>% filter(value>=0)
View(a)
setwd("~/Proyectos R/Cursos/Data Science 6. Wrangling")
